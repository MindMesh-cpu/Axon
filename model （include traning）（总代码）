import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
from scipy.signal import butter, lfilter

# 基础预处理：带通滤波 
def butter_bandpass(lowcut, highcut, fs, order=4):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return b, a

def bandpass_filter(data, lowcut=1, highcut=40, fs=250, order=4):
    # data shape: (..., channels, time)
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    return lfilter(b, a, data, axis=-1)

# 数据集 
class EEGDataset(Dataset):
    def __init__(self, npz_path, train=True, mean=None, std=None, fs=250):
        data = np.load(npz_path)
        if train:
            self.X = data['X_train']
            self.y = data['y_train']
        else:
            self.X = data['X_test']
            self.y = data['y_test']

        # 带通滤波
        self.X = np.array([bandpass_filter(x, lowcut=1, highcut=40, fs=fs) for x in self.X])
        self.X = self.X.astype(np.float32)

        # 标准化
        if train:
            self.mean = self.X.mean(axis=(0, 2), keepdims=True)
            self.std = self.X.std(axis=(0, 2), keepdims=True)
            self.std[self.std == 0] = 1e-6
        else:
            assert mean is not None and std is not None, "测试集需要训练集的mean和std"
            self.mean = mean
            self.std = std

        self.X = (self.X - self.mean) / self.std
        self.X = torch.tensor(self.X, dtype=torch.float32)
        self.y = torch.tensor(self.y, dtype=torch.long)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# 数据增强（训练时酌情摸奖）
def mixup_data(x, y, alpha=0.2):
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def channel_dropout(x, p=0.1):
    mask = (torch.rand(x.shape[0], x.shape[1], 1, device=x.device) > p).float()
    return x * mask

def random_time_crop(x, window_size=600):
    T = x.shape[2]
    if T <= window_size:
        return x
    start = np.random.randint(0, T - window_size)
    return x[:, :, start:start + window_size]

def add_noise(x, std=0.01):
    return x + std * torch.randn_like(x)

# 模型组件
class DeepStackedTemporalConv(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv1d(channels, channels, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(channels)
        self.conv2 = nn.Conv1d(channels, channels, kernel_size=5, padding=2)
        self.bn2 = nn.BatchNorm1d(channels)
        self.relu = nn.ReLU()

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += identity
        out = self.relu(out)
        return out

class DynamicMultiScaleTimeConv(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv1d(channels, channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm1d(channels)
        self.conv2 = nn.Conv1d(channels, channels, kernel_size=7, padding=3)
        self.bn2 = nn.BatchNorm1d(channels)
        self.relu = nn.ReLU()

    def forward(self, x):
        identity = x
        out1 = self.conv1(x)
        out1 = self.bn1(out1)
        out1 = self.relu(out1)
        out2 = self.conv2(x)
        out2 = self.bn2(out2)
        out2 = self.relu(out2)
        out = out1 + out2 + identity
        out = self.relu(out)
        return out

class DynamicChannelFusion(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv1d(channels, channels, kernel_size=1)
        self.bn = nn.BatchNorm1d(channels)
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        out = self.relu(out)
        return out

class ChannelAttention(nn.Module):
    def __init__(self, channels, reduction=4, dropout=0.1):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.LayerNorm(channels // reduction),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)

class TemporalAttention(nn.Module):
    def __init__(self, channels, hidden_dim=128):
        super().__init__()
        self.W = nn.Linear(channels, hidden_dim)
        self.norm = nn.LayerNorm(hidden_dim)
        self.v = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x_perm = x.permute(0, 2, 1)
        e = torch.tanh(self.norm(self.W(x_perm)))
        e = self.v(e).squeeze(-1)
        alpha = F.softmax(e, dim=1)
        out = (x_perm * alpha.unsqueeze(-1)).sum(dim=1)
        return out

class Classifier(nn.Module):
    def __init__(self, input_dim, num_classes, dropout=0.2):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.LayerNorm(input_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.fc(x)

# 主模型
class FinalEEGNet(nn.Module):
    def __init__(self, channels=60, num_classes=4):
        super().__init__()
        self.dynamic_time = DynamicMultiScaleTimeConv(channels)
        self.stacked_time = DeepStackedTemporalConv(channels)
        self.channel_mix = DynamicChannelFusion(channels)

        self.bn_time = nn.BatchNorm1d(channels)
        self.bn_channel = nn.BatchNorm1d(channels)

        # 融合
        self.fusion_conv = nn.Conv1d(channels*2, channels, kernel_size=1, bias=False)
        self.fusion_bn = nn.BatchNorm1d(channels)
        self.fusion_relu = nn.ReLU()

        self.channel_att = ChannelAttention(channels)
        self.temporal_att = TemporalAttention(channels)

        self.classifier = Classifier(input_dim=channels, num_classes=num_classes)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, (nn.Conv1d, nn.Linear)):
            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    def forward(self, x):
        identity = x
        x1 = self.dynamic_time(x)
        x2 = self.stacked_time(x)
        x_time = self.bn_time(x1 + x2) + identity
        x_channel = self.bn_channel(self.channel_mix(x)) + identity
        x_cat = torch.cat([x_time, x_channel], dim=1)
        x_fused = self.fusion_conv(x_cat)
        x_fused = self.fusion_bn(x_fused)
        x_fused = self.fusion_relu(x_fused)
        x_att = self.channel_att(x_fused) + x_fused
        x_att = self.temporal_att(x_att)
        out = self.classifier(x_att)
        return out

# 训练与验证
def train(model, train_loader, val_loader, device, num_epochs=50, lr=1e-3):
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
    criterion = nn.CrossEntropyLoss()
    best_acc = 0.0
    for epoch in range(1, num_epochs+1):
        model.train()
        train_loss, train_correct, train_total = 0.0, 0, 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            # 数据增强
            x = random_time_crop(x, window_size=int(x.size(2)*0.8))
            x = channel_dropout(x, p=0.2)
            x = add_noise(x, std=0.01)
            x, y_a, y_b, lam = mixup_data(x, y, alpha=0.4)

            optimizer.zero_grad()
            logits = model(x)
            loss = lam * criterion(logits, y_a) + (1 - lam) * criterion(logits, y_b)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()

            train_loss += loss.item() * y.size(0)
            _, preds = torch.max(logits, 1)
            train_correct += (preds == y_a).sum().item()
            train_total += y.size(0)
        scheduler.step()
        train_loss /= train_total
        train_acc = 100 * train_correct / train_total

        # 验证
        model.eval()
        val_loss, val_correct, val_total = 0.0, 0, 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                logits = model(x)
                loss = criterion(logits, y)
                val_loss += loss.item() * y.size(0)
                _, preds = torch.max(logits, 1)
                val_correct += (preds == y).sum().item()
                val_total += y.size(0)
        val_loss /= val_total
        val_acc = 100 * val_correct / val_total

        print(f"Epoch {epoch}/{num_epochs} | "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | "
              )

        # 保存最优模型
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_modeleegnet.pth')
    print(f"最佳验证准确率: {best_acc:.2f}%")

# 主程序
if __name__ == "__main__":
    npz_path = '.venv/k3b.gdf/IIIa_data.npz'  
    batch_size = 64
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # 加载数据
    full_train = EEGDataset(npz_path, train=True)
    val_size = int(len(full_train) * 0.1)
    train_size = len(full_train) - val_size
    train_set, val_set = random_split(full_train, [train_size, val_size])
    val_set.dataset.mean = full_train.mean
    val_set.dataset.std = full_train.std
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)

    model = FinalEEGNet(channels=full_train.X.shape[1], num_classes=len(torch.unique(full_train.y))).to(device)
    train(model, train_loader, val_loader, device, num_epochs=200, lr=1e-3)

    # 测试
    test_set = EEGDataset(npz_path, train=False, mean=full_train.mean, std=full_train.std)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)
    model.load_state_dict(torch.load('best_modeleegnet.pth', map_location=device))
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            _, preds = torch.max(logits, 1)
            total += y.size(0)
            correct += (preds == y).sum().item()
    print(f"Test Accuracy: {100*correct/total:.2f}%")
