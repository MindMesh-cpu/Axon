class TemporalAttention(nn.Module):
    def __init__(self, channels, hidden_dim=128):
        super().__init__()
        self.W = nn.Linear(channels, hidden_dim)
        self.norm = nn.LayerNorm(hidden_dim)
        self.v = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x_perm = x.permute(0, 2, 1)
        e = torch.tanh(self.norm(self.W(x_perm)))
        e = self.v(e).squeeze(-1)
        alpha = F.softmax(e, dim=1)
        out = (x_perm * alpha.unsqueeze(-1)).sum(dim=1)
        return out

class Classifier(nn.Module):
    def __init__(self, input_dim, num_classes, dropout=0.2):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.LayerNorm(input_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.fc(x)

